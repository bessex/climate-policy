{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-03-21T00:20:11.768632Z",
     "end_time": "2023-03-21T00:20:11.783836Z"
    }
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "import json\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from math import isnan\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# import bias csv\n",
    "bias = pd.read_csv('../models/allsides_data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-21T00:02:46.576947Z",
     "end_time": "2023-03-21T00:02:46.603881Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "source_name_dict = {}\n",
    "def get_source_name(url):\n",
    "    if url in source_name_dict:\n",
    "        return source_name_dict[url]\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.find(\"title\").get_text()\n",
    "        source_name = title.split('-', 1)[0].strip()\n",
    "    except Exception as e:\n",
    "        source_name = ' '.join(urlparse(url).hostname.replace('www.', '').split('.')[:-1])\n",
    "\n",
    "    source_name_dict[url] = source_name.lower()\n",
    "\n",
    "    return source_name.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-21T00:02:47.346742Z",
     "end_time": "2023-03-21T00:02:47.376027Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_source_name_allsides(url):\n",
    "    # raise an error if the url is not an allsides url\n",
    "    if 'allsides.com' not in url:\n",
    "        raise ValueError('url must be an allsides url')\n",
    "\n",
    "    response = requests.get(url, timeout=5)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    external_link = soup.find('a', class_='external-link')\n",
    "\n",
    "    if external_link:\n",
    "        return get_source_name(external_link['href'])\n",
    "    else:\n",
    "        raise ValueError('no external link found')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-21T00:02:49.698842Z",
     "end_time": "2023-03-21T00:02:49.727198Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Build a dictionary of source names and the corresponding bias rating from allsides\n",
    "source_bias = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-21T00:03:09.940061Z",
     "end_time": "2023-03-21T00:03:09.958107Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    print(f'Processing {row[\"news_source\"]} ({row.name+1}/{len(bias)}).')\n",
    "\n",
    "    try:\n",
    "        src_name = get_source_name_allsides(row['url'])\n",
    "    except Exception as e:\n",
    "        src_name = row['news_source'].lower()\n",
    "\n",
    "    rating = row['rating_num']\n",
    "    source_bias[src_name] = rating if not isnan(rating) else None\n",
    "\n",
    "    print(f'Finished ({row.name+1}/{len(bias)}).')\n",
    "    return src_name, row['rating_num']\n",
    "\n",
    "# do 5 requests in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Submit each row to the executor and store the resulting futures\n",
    "    futures = {executor.submit(process_row, row): row for _, row in bias.iterrows()}\n",
    "\n",
    "    # Wait for all the tasks to complete\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(f'An error occurred: {e}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# dump as json\n",
    "with open('../models/source_bias.json', 'w') as f:\n",
    "    json.dump(source_bias, f, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-20T23:54:09.146202Z",
     "end_time": "2023-03-20T23:54:09.161151Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# load source bias\n",
    "if source_bias == {}:\n",
    "    with open('../models/source_bias.json', 'r') as f:\n",
    "        source_bias = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-21T00:03:17.303001Z",
     "end_time": "2023-03-21T00:03:17.321204Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_bias(source_name):\n",
    "    if source_name in source_bias:\n",
    "        return source_bias[source_name]\n",
    "    else:\n",
    "        sn_words = source_name.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        for key, bias in source_bias.items():\n",
    "            # remove punctuation\n",
    "            key_no_punct = key.translate(str.maketrans('', '', string.punctuation))\n",
    "            if all([word in key_no_punct for word in sn_words]) and bias is not None:\n",
    "                return bias\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-21T00:32:02.903187Z",
     "end_time": "2023-03-21T00:32:02.921244Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1000/43881\n",
      "Finished 2000/43881\n",
      "Finished 3000/43881\n",
      "Finished 4000/43881\n",
      "Finished 5000/43881\n",
      "Finished 6000/43881\n",
      "Finished 7000/43881\n",
      "Finished 8000/43881\n",
      "Finished 9000/43881\n",
      "Finished 10000/43881\n",
      "Finished 11000/43881\n",
      "Finished 12000/43881\n",
      "Finished 13000/43881\n",
      "Finished 14000/43881\n",
      "Finished 15000/43881\n",
      "Finished 16000/43881\n",
      "Finished 17000/43881\n",
      "Finished 18000/43881\n",
      "Finished 19000/43881\n",
      "Finished 20000/43881\n",
      "Finished 21000/43881\n",
      "Finished 22000/43881\n",
      "Finished 23000/43881\n",
      "Finished 24000/43881\n",
      "Finished 25000/43881\n",
      "Finished 26000/43881\n",
      "Finished 27000/43881\n",
      "Finished 28000/43881\n",
      "Finished 29000/43881\n",
      "Finished 30000/43881\n",
      "Finished 31000/43881\n",
      "Finished 32000/43881\n",
      "Finished 33000/43881\n",
      "Finished 34000/43881\n",
      "Finished 35000/43881\n",
      "Finished 36000/43881\n",
      "Finished 37000/43881\n",
      "Finished 38000/43881\n",
      "Finished 39000/43881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bessex\\AppData\\Local\\mambaforge\\envs\\climate-policy\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 40000/43881\n",
      "Finished 41000/43881\n",
      "Finished 42000/43881\n",
      "Finished 43000/43881\n",
      "Finished 43881/43881\n"
     ]
    }
   ],
   "source": [
    "# now we load all the documents into memory and associate a bias rating if one exists\n",
    "with open('../data/documents_1.json', 'r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "with open('../data/documents_2.json', 'r') as f:\n",
    "    documents.extend(json.load(f))\n",
    "\n",
    "with open('../data/documents_3.json', 'r') as f:\n",
    "    documents.extend(json.load(f))\n",
    "\n",
    "counter_lock = threading.Lock()\n",
    "counter = 0\n",
    "\n",
    "# create documents dictionary for easy lookup and to remove duplicates\n",
    "documents_dict = {doc['document_id']: doc for doc in documents}\n",
    "\n",
    "def process_document(doc):\n",
    "    global counter\n",
    "\n",
    "    # get the source name from the url\n",
    "    source_name = get_source_name(doc['source']['home_page_url'])\n",
    "\n",
    "    # get the bias rating from the source_bias dictionary\n",
    "    bias_rating = get_bias(source_name)\n",
    "\n",
    "    # add bias rating (or None) to the document\n",
    "    doc['bias_rating'] = bias_rating\n",
    "\n",
    "    with counter_lock:\n",
    "        counter += 1\n",
    "        if counter % 1000 == 0 or counter == len(documents_dict):\n",
    "            print(f'Finished {counter}/{len(documents_dict)}')\n",
    "    return doc\n",
    "\n",
    "# do 5 requests in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:\n",
    "    # Submit each doc to the executor and store the resulting futures\n",
    "    futures = {executor.submit(process_document, doc): doc for doc in documents_dict.values()}\n",
    "\n",
    "    # Wait for all the tasks to complete\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(f'An error occurred: {e}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-21T00:03:19.289923Z",
     "end_time": "2023-03-21T00:06:21.378170Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "with open('../data/documents_bias.json', 'w') as f:\n",
    "    json.dump(documents_dict, f, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-21T00:32:57.850167Z",
     "end_time": "2023-03-21T00:33:01.635249Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

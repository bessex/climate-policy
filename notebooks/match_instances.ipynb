{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import aylien_news_api\n",
    "from aylien_news_api.rest import ApiException\n",
    "from pprint import pprint\n",
    "import copy\n",
    "import dotenv\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "import time\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def begin_json_array_in_file(file_path: str) -> bool:\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r+') as f:\n",
    "            if (start := f.read(1)) == '[':\n",
    "                print(\"file_path non-empty.\")\n",
    "            elif start:\n",
    "                print(\"file_path non-empty, but doesn't start with '['. Stopping.\")\n",
    "                return False\n",
    "            else:\n",
    "                print(\"file_path empty. Will overwrite\")\n",
    "                f.write('[\\n')\n",
    "\n",
    "        with open(file_path, 'rb+') as f:\n",
    "            f.seek(-2, os.SEEK_END)\n",
    "            final = str(f.read(1), 'utf-8')\n",
    "            f.seek(-1, os.SEEK_CUR)\n",
    "            if final == ']':\n",
    "                print(\"file_path ends with ']'. Removing.\")\n",
    "                f.truncate()\n",
    "                f.write(bytes(',\\n', 'utf-8'))\n",
    "            elif final == ',' or final == '[':\n",
    "                print(\"file_path ends with \" + final + \". Will append to end.\")\n",
    "            else:\n",
    "                print(\"file_path doesn't end with ',' or ']'. Check that it's valid JSON and that the file ends with newline. Stopping.\")\n",
    "                return False\n",
    "    else:\n",
    "        with open(file_path, 'w') as f:\n",
    "            print(\"file_path doesn't exist. Will create.\")\n",
    "            f.write('[\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def end_json_array_in_file(file_path: str):\n",
    "    with open(file_path, 'rb+') as f:\n",
    "        # Remove trailing comma if present\n",
    "        f.seek(-2, os.SEEK_END)\n",
    "        if str(f.read(1), 'utf-8') == ',':\n",
    "            print(\"Removing trailing comma.\")\n",
    "            f.seek(-1, os.SEEK_CUR)\n",
    "            f.truncate()\n",
    "            f.write(bytes('\\n]\\n', 'utf-8'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def match_terms(documents: dict, instances_file: str, terms: list, logging=False, processed=None) -> Tuple[set, list]:\n",
    "    if not processed:\n",
    "        processed = set()\n",
    "\n",
    "    if logging:\n",
    "        print('Loading spacy model...', end=' ')\n",
    "\n",
    "    nlp = English()\n",
    "    tokenizer = nlp.tokenizer\n",
    "\n",
    "    if logging:\n",
    "        print('done')\n",
    "\n",
    "    ##############################\n",
    "\n",
    "    if logging:\n",
    "        print('Pre-processing k-grams...', end=' ')\n",
    "\n",
    "    # Preprocess the k-grams and create a dictionary that maps each unique starting token\n",
    "    # to the set of k-grams that contain that token\n",
    "    # The purpose of this is to speed up the matching process\n",
    "    k_grams_dict = {}\n",
    "    k_grams_tokens = {}\n",
    "    for k_gram in terms:\n",
    "        k_gram_lower = k_gram.lower()\n",
    "        k_gram_tokens = tokenizer(k_gram_lower)\n",
    "        start_token = k_gram_tokens[0].text\n",
    "        if start_token not in k_grams_dict:\n",
    "            k_grams_dict[start_token] = set()\n",
    "        k_grams_dict[start_token].add(k_gram_lower)\n",
    "        k_grams_tokens[k_gram_lower] = k_gram_tokens\n",
    "\n",
    "    if logging:\n",
    "        print('done')\n",
    "\n",
    "    ##############################\n",
    "\n",
    "    begin_json_array_in_file(instances_file)\n",
    "\n",
    "    ##############################\n",
    "\n",
    "    # instance schema:\n",
    "    # {\n",
    "    #     \"document_id\": \"document_id\",\n",
    "    #     \"instance_id\": \"instance_id\",\n",
    "    #     \"match\": \"match\",\n",
    "    #     \"start\": \"start\",\n",
    "    #     \"end\": \"end\"\n",
    "    # }\n",
    "    # Note that start is inclusive and end is exclusive,\n",
    "    # and indexing is 0-based on tokens (not characters)\n",
    "\n",
    "    all_instances = []\n",
    "    num_documents_processed = 0\n",
    "\n",
    "    for document_id, document in documents.items():\n",
    "        doc_instances = []\n",
    "\n",
    "        if document_id in processed:\n",
    "            continue\n",
    "\n",
    "        if logging:\n",
    "            print(f'Processing document {document_id}...', end=' ')\n",
    "\n",
    "        # Preprocess the document\n",
    "        doc_tokens = tokenizer(document['body'].lower())\n",
    "\n",
    "        # Iterate over the tokens in the document\n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            # If the token is in the dictionary of k-grams\n",
    "            if token.text in k_grams_dict:\n",
    "                for k_gram in k_grams_dict[token.text]:\n",
    "                    if i+len(k_grams_tokens[k_gram]) > len(doc_tokens):\n",
    "                        # TODO: watch for off-by-one error here\n",
    "                        continue\n",
    "\n",
    "                    # Check if the k-gram matches the document\n",
    "                    k_gram_tokens = k_grams_tokens[k_gram]\n",
    "                    if not all(doc_tokens[i+j].text == k_gram_tokens[j].text for j, _ in enumerate(k_gram_tokens)):\n",
    "                        continue\n",
    "\n",
    "                    # If the k-gram matches, add an instance\n",
    "                    instance = {\n",
    "                        'document_id': document_id,\n",
    "                        'instance_id': f'{document_id}_{len(doc_instances)}',\n",
    "                        'match': k_gram,\n",
    "                        'start': i,\n",
    "                        'end': i+len(k_gram_tokens)\n",
    "                    }\n",
    "\n",
    "                    doc_instances.append(instance)\n",
    "\n",
    "        all_instances.extend(doc_instances)\n",
    "\n",
    "        with open(instances_file, 'a') as f:\n",
    "            for instance in doc_instances:\n",
    "                f.write(json.dumps(instance) + ',\\n')\n",
    "\n",
    "        num_documents_processed += 1\n",
    "        processed.add(document_id)\n",
    "\n",
    "        if logging:\n",
    "            print('done (%d/%d)' % (num_documents_processed, len(documents)))\n",
    "            print(f'Found {len(doc_instances)} instances.')\n",
    "\n",
    "    end_json_array_in_file(instances_file)\n",
    "    print(f'Found {len(all_instances)} instances in total.')\n",
    "\n",
    "    return processed, all_instances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def get_terms_from_bills(bills_file) -> list:\n",
    "    with open(bills_file, 'r') as f:\n",
    "        bills = json.load(f)\n",
    "\n",
    "    terms = []\n",
    "\n",
    "    for bill in bills.values():\n",
    "        terms.append(bill['short_title'])\n",
    "\n",
    "        # bill['bill_number'] might look like 'H.R.1' or 'S.312' but we want 'H.R. 1' or 'S. 312'\n",
    "        # if the bill number already has a space after the period, don't add another one\n",
    "        # we can rfind the first period and insert a space after it if necessary\n",
    "        bill_number = bill['bill_number']\n",
    "        dot_index = bill_number.rfind('.')\n",
    "        if bill_number[dot_index+1] != ' ':\n",
    "            bill_number = bill_number[:dot_index] + '. ' + bill_number[dot_index+1:]\n",
    "\n",
    "        terms.append(bill_number)\n",
    "\n",
    "        # sometimes it's even 'H.RES.' or 'S.RES.'--in these cases we should also\n",
    "        # add 'H. RES.', 'S. RES.', 'H.R.' and 'S.R.'\n",
    "        if 'RES.' in bill_number:\n",
    "            terms.append(bill_number.replace('RES.', 'R.'))\n",
    "            terms.append(bill_number.replace('RES.', ' RES.'))\n",
    "\n",
    "        # in theory there could be other cases, but we'll just add them manually if we find them\n",
    "\n",
    "        # TODO: for debugging only....\n",
    "        # bill_number_digits = bill_number.split(' ')[-1]\n",
    "        # terms.append(bill_number_digits)\n",
    "\n",
    "    return terms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "instances_file = '../data/all_instances.json'\n",
    "bills_file = '../data/bill_data.json'\n",
    "\n",
    "terms = get_terms_from_bills(bills_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents... "
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/documents_1.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoading documents...\u001B[39m\u001B[38;5;124m'\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../data/documents_1.json\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m      4\u001B[0m     stories_1 \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/documents_2.json\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "File \u001B[0;32m~/miniforge3/envs/climate-policy/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    277\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    278\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    280\u001B[0m     )\n\u001B[0;32m--> 282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/documents_1.json'"
     ]
    }
   ],
   "source": [
    "print('Loading documents...', end=' ')\n",
    "\n",
    "with open('../data/documents_1.json', 'r') as f:\n",
    "    stories_1 = json.load(f)\n",
    "\n",
    "with open('../data/documents_2.json', 'r') as f:\n",
    "    stories_2 = json.load(f)\n",
    "\n",
    "with open('../data/documents_3.json', 'r') as f:\n",
    "    stories_3 = json.load(f)\n",
    "\n",
    "stories = stories_1 + stories_2 + stories_3\n",
    "\n",
    "documents = {story['document_id']: story for story in stories}\n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m processed, instances \u001B[38;5;241m=\u001B[39m match_terms(\u001B[43mdocuments\u001B[49m, instances_file, terms, logging\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "processed, instances = match_terms(documents, instances_file, terms, logging=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'instances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m nlp \u001B[38;5;241m=\u001B[39m English()\n\u001B[1;32m      2\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m nlp\u001B[38;5;241m.\u001B[39mtokenizer\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43minstances\u001B[49m[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmatch\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(tokenizer(instances[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmatch\u001B[39m\u001B[38;5;124m'\u001B[39m]))\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(tokenizer(documents[instances[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdocument_id\u001B[39m\u001B[38;5;124m'\u001B[39m]][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m])[instances[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstart\u001B[39m\u001B[38;5;124m'\u001B[39m]:instances[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m'\u001B[39m]])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'instances' is not defined"
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "tokenizer = nlp.tokenizer\n",
    "print(instances[0]['match'])\n",
    "print(tokenizer(instances[0]['match']))\n",
    "print(tokenizer(documents[instances[0]['document_id']]['body'])[instances[0]['start']:instances[0]['end']])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import aylien_news_api\n",
    "from aylien_news_api.rest import ApiException\n",
    "from pprint import pprint\n",
    "import copy\n",
    "import dotenv\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Configure API key authorization: app_id\n",
    "app_id = dotenv.get_key('../.env', 'AYLIEN_APP_ID')\n",
    "app_key = dotenv.get_key('../.env', 'AYLIEN_APP_KEY')\n",
    "endpoint = dotenv.get_key('../.env', 'AYLIEN_ENDPOINT')\n",
    "\n",
    "configuration = aylien_news_api.Configuration()\n",
    "configuration.api_key['X-AYLIEN-NewsAPI-Application-ID'] = app_id\n",
    "configuration.api_key['X-AYLIEN-NewsAPI-Application-Key'] = app_key\n",
    "configuration.host = endpoint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "client = aylien_news_api.ApiClient(configuration)\n",
    "api_instance = aylien_news_api.DefaultApi(client)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Fetch stories using AYLIEN News API\n",
    "def fetch_stories_to_file(opts, append_to_file, limit=None):\n",
    "    response = None\n",
    "    fetched_stories = []\n",
    "    stories = None\n",
    "    RETRY_COUNT = 5\n",
    "    MAX_PER_PAGE = 100\n",
    "    retry = 0\n",
    "\n",
    "    def _end_of_file():\n",
    "        if append_to_file:\n",
    "            with open(append_to_file, 'rb+') as f:\n",
    "                # Remove trailing comma if present\n",
    "                f.seek(-2, os.SEEK_END)\n",
    "                if str(f.read(1), 'utf-8') == ',':\n",
    "                    print(\"Removing trailing comma.\")\n",
    "                    f.seek(-1, os.SEEK_CUR)\n",
    "                    f.truncate()\n",
    "                    f.write(bytes('\\n]\\n', 'utf-8'))\n",
    "\n",
    "    if append_to_file and os.path.exists(append_to_file):\n",
    "        with open(append_to_file, 'r+') as f:\n",
    "            if (start := f.read(1)) == '[':\n",
    "                print(\"append_to non-empty.\")\n",
    "            elif start:\n",
    "                print(\"append_to non-empty, but doesn't start with '['. Stopping.\")\n",
    "                return fetched_stories\n",
    "            else:\n",
    "                print(\"append_to empty. Will overwrite\")\n",
    "                f.write('[\\n')\n",
    "\n",
    "        with open(append_to_file, 'rb+') as f:\n",
    "            f.seek(-2, os.SEEK_END)\n",
    "            final = str(f.read(1), 'utf-8')\n",
    "            f.seek(-1, os.SEEK_CUR)\n",
    "            if final == ']':\n",
    "                print(\"append_to ends with ']'. Removing.\")\n",
    "                f.truncate()\n",
    "                f.write(bytes(',\\n', 'utf-8'))\n",
    "            elif final == ',' or final == '[':\n",
    "                print(\"append_to ends with \" + final + \". Will append to end.\")\n",
    "            else:\n",
    "                print(\"append_to doesn't end with ',' or ']'. Check that it's valid JSON and that the file ends with newline. Stopping.\")\n",
    "                return fetched_stories\n",
    "    elif append_to_file:\n",
    "        with open(append_to_file, 'w') as f:\n",
    "            print(\"append_to doesn't exist. Will create.\")\n",
    "            f.write('[\\n')\n",
    "\n",
    "    while stories is None or len(stories) == (opts.get('per_page') or MAX_PER_PAGE):\n",
    "        if limit and len(fetched_stories) >= limit:\n",
    "            print('Reached limit of %d stories' % limit)\n",
    "            print('Next page cursor: %s' % response.next_page_cursor or 'None')\n",
    "            _end_of_file()\n",
    "            return fetched_stories\n",
    "        try:\n",
    "            response = api_instance.list_stories(**opts)\n",
    "        except ApiException as e:\n",
    "            if e.status == 429:\n",
    "                if int(e.headers['x-ratelimit-volume-remaining']) == 0:\n",
    "                    reset = e.headers('x-ratelimit-volume-reset')\n",
    "                    print('Monthly rate limit exceeded. Wait until reset at %s' % reset)\n",
    "                    print('Next page cursor: %s' % response.next_page_cursor or 'None')\n",
    "                    _end_of_file()\n",
    "                    return fetched_stories\n",
    "                if int(e.headers['x-ratelimit-remaining']) == 0:\n",
    "                    print('1-Minute rate limit exceeded. Waiting 60 seconds...')\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "            else:\n",
    "                print(\"Exception when calling DefaultApi->list_stories (see below)\")\n",
    "                print(\"Retry %d/%d\" % (retry, RETRY_COUNT))\n",
    "                print(\"Status: %d\" % e.status)\n",
    "                print(\"Reason: %s\" % e.reason)\n",
    "                if retry < RETRY_COUNT:\n",
    "                    print(\"Retrying in 10 seconds...\")\n",
    "                    retry += 1\n",
    "                    time.sleep(10)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Giving up. Check end of file '%s' for partial results.\" % append_to_file)\n",
    "                    print('Next page cursor: %s' % response.next_page_cursor or 'None')\n",
    "                    _end_of_file()\n",
    "                    return fetched_stories\n",
    "\n",
    "        if append_to_file:\n",
    "            with open(append_to_file, 'a') as f:\n",
    "                for story in response.stories:\n",
    "                    sentiment_dict = {'body_polarity': story.sentiment.body.polarity if story.sentiment.body else None,\n",
    "                                      'body_score': story.sentiment.body.score if story.sentiment.body else None,\n",
    "                                      'title_polarity': story.sentiment.title.polarity if story.sentiment.title else None,\n",
    "                                      'title_score': story.sentiment.title.score if story.sentiment.title else None}\n",
    "\n",
    "                    source_dict = {\n",
    "                        'description': story.source.description if story.source else None,\n",
    "                        'discriminator': story.source.discriminator if story.source else None,\n",
    "                        'domain': story.source.domain if story.source else None,\n",
    "                        'home_page_url': story.source.home_page_url if story.source else None,\n",
    "                        'id': story.source.id if story.source else None,\n",
    "                        'links_in_count': story.source.links_in_count if story.source else None\n",
    "                    }\n",
    "\n",
    "                    story_dict = {'document_id': story.id, 'body': story.body, 'date': story.published_at.isoformat(),\n",
    "                                  'sentiment': sentiment_dict, 'source': source_dict}\n",
    "                    fetched_stories.append(story_dict)\n",
    "                    json.dump(story_dict, f, indent=4)\n",
    "                    f.write(',\\n')\n",
    "\n",
    "        stories = response.stories\n",
    "        opts['cursor'] = response.next_page_cursor\n",
    "\n",
    "        print(f'Fetched {len(stories)} stories. Total: {len(fetched_stories)} so far.')\n",
    "\n",
    "    _end_of_file()\n",
    "    return fetched_stories\n",
    "\n",
    "# Alternate: Fetch stories using pure HTTP requests\n",
    "def fetch_news(api_id, api_key, opts):\n",
    "    base_url = 'https://api.aylien.com/news/stories'\n",
    "    headers = {\n",
    "        'X-AYLIEN-NewsAPI-Application-ID': api_id,\n",
    "        'X-AYLIEN-NewsAPI-Application-Key': api_key\n",
    "    }\n",
    "    params = {\n",
    "        'published_at.start': opts['published_at_start'],\n",
    "        'published_at.end': opts['published_at_end'],\n",
    "        # 'source_locations.country': ','.join(opts['source_locations_country']),\n",
    "        # 'language': ','.join(opts['language']),\n",
    "        'sort_by': opts['sort_by'],\n",
    "        # 'sort_direction': opts['sort_direction'],\n",
    "        'per_page': opts['per_page'],\n",
    "    }\n",
    "    if 'aql' in opts:\n",
    "        params['aql'] = opts['aql']\n",
    "    elif 'text' in opts:\n",
    "        params['text'] = opts['text']\n",
    "\n",
    "    response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error when calling AYLIEN News API: {response.status_code}, {response.text}\")\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_terms_from_bills(bills_file) -> list:\n",
    "    with open(bills_file, 'r') as f:\n",
    "        bills = json.load(f)\n",
    "\n",
    "    terms = []\n",
    "\n",
    "    for bill in bills.values():\n",
    "        terms.append(bill['short_title'])\n",
    "\n",
    "        # bill['bill_number'] might look like 'H.R.1' or 'S.312' but we want 'H.R. 1' or 'S. 312'\n",
    "        # if the bill number already has a space after the period, don't add another one\n",
    "        # we can rfind the first period and insert a space after it if necessary\n",
    "        bill_number = bill['bill_number']\n",
    "        dot_index = bill_number.rfind('.')\n",
    "        if bill_number[dot_index+1] != ' ':\n",
    "            bill_number = bill_number[:dot_index] + '. ' + bill_number[dot_index+1:]\n",
    "\n",
    "        terms.append(bill_number)\n",
    "\n",
    "        # sometimes it's even 'H.RES.' or 'S.RES.'--in these cases we should also\n",
    "        # add 'H. RES.', 'S. RES.', 'H.R.' and 'S.R.'\n",
    "        if 'RES.' in bill_number:\n",
    "            terms.append(bill_number.replace('RES.', 'R.'))\n",
    "            terms.append(bill_number.replace('RES.', ' RES.'))\n",
    "\n",
    "        # in theory there could be other cases, but we'll just add them manually if we find them\n",
    "\n",
    "        # TODO: for debugging only....\n",
    "        # bill_number_digits = bill_number.split(' ')[-1]\n",
    "        # terms.append(bill_number_digits)\n",
    "\n",
    "    return terms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# generate AYLIEN Query Language (AQL, based on Lucene) query from keywords\n",
    "keywords_file = '../models/story_keywords.txt'\n",
    "with open(keywords_file, 'r') as f:\n",
    "    keywords = f.read().splitlines()\n",
    "\n",
    "climate_keywords_aql = 'text:(' + ' OR '.join([f'\"{keyword}\"' for keyword in keywords]) + ')'\n",
    "\n",
    "# generate AQL query from bill info\n",
    "bill_data_file = '../data/bill_data.json'\n",
    "bill_terms = get_terms_from_bills(bill_data_file)\n",
    "\n",
    "half = len(bill_terms) // 2\n",
    "\n",
    "bill_aql_1 = 'text:(' + ' OR '.join([f'\"{term}\"' for term in bill_terms[:half]]) + ')'\n",
    "bill_aql_2 = 'text:(' + ' OR '.join([f'\"{term}\"' for term in bill_terms[half:]]) + ')'\n",
    "\n",
    "# An attempt at a more targeted query\n",
    "bill_aql_3 = 'text:(' + ' OR '.join(f'\"{term}\"' for term in get_terms_from_bills(bill_data_file)) + ')'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# set options\n",
    "opts_1 = {\n",
    "    'aql': bill_aql_1,\n",
    "    'published_at_start': 'NOW-27MONTH',\n",
    "    'published_at_end': 'NOW-1DAY',\n",
    "    'source_locations_country': ['US'],\n",
    "    'language': ['en'],\n",
    "    'sort_by': 'relevance',\n",
    "    # 'sort_direction': 'asc',\n",
    "    'per_page': 100,\n",
    "}\n",
    "\n",
    "opts_2 = copy.deepcopy(opts_1)\n",
    "opts_2['aql'] = bill_aql_2\n",
    "\n",
    "opts_3 = copy.deepcopy(opts_1)\n",
    "opts_3['aql'] = bill_aql_3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 10000 stories to file ../data/documents_2.json. Starting at page OC4yMzIyODU1LDM2MTg4NTE0Nzc=...\n",
      "append_to non-empty.\n",
      "append_to ends with ']'. Removing.\n",
      "Fetched 100 stories. Total: 100 so far.\n",
      "Fetched 100 stories. Total: 200 so far.\n",
      "Fetched 100 stories. Total: 300 so far.\n",
      "Fetched 100 stories. Total: 400 so far.\n",
      "Fetched 100 stories. Total: 500 so far.\n",
      "Fetched 100 stories. Total: 600 so far.\n",
      "Fetched 100 stories. Total: 700 so far.\n",
      "Fetched 100 stories. Total: 800 so far.\n",
      "Fetched 100 stories. Total: 900 so far.\n",
      "Fetched 100 stories. Total: 1000 so far.\n",
      "Fetched 100 stories. Total: 1100 so far.\n",
      "Fetched 100 stories. Total: 1200 so far.\n",
      "Fetched 100 stories. Total: 1300 so far.\n",
      "Fetched 100 stories. Total: 1400 so far.\n",
      "Fetched 100 stories. Total: 1500 so far.\n",
      "Fetched 100 stories. Total: 1600 so far.\n",
      "Fetched 100 stories. Total: 1700 so far.\n",
      "Fetched 100 stories. Total: 1800 so far.\n",
      "Fetched 100 stories. Total: 1900 so far.\n",
      "Fetched 100 stories. Total: 2000 so far.\n",
      "Fetched 100 stories. Total: 2100 so far.\n",
      "Fetched 100 stories. Total: 2200 so far.\n",
      "Fetched 100 stories. Total: 2300 so far.\n",
      "Fetched 100 stories. Total: 2400 so far.\n",
      "Fetched 100 stories. Total: 2500 so far.\n",
      "Fetched 100 stories. Total: 2600 so far.\n",
      "Fetched 100 stories. Total: 2700 so far.\n",
      "Fetched 100 stories. Total: 2800 so far.\n",
      "Fetched 100 stories. Total: 2900 so far.\n",
      "Fetched 100 stories. Total: 3000 so far.\n",
      "Fetched 100 stories. Total: 3100 so far.\n",
      "Fetched 100 stories. Total: 3200 so far.\n",
      "Fetched 100 stories. Total: 3300 so far.\n",
      "Fetched 100 stories. Total: 3400 so far.\n",
      "Fetched 100 stories. Total: 3500 so far.\n",
      "Fetched 100 stories. Total: 3600 so far.\n",
      "Fetched 100 stories. Total: 3700 so far.\n",
      "Fetched 100 stories. Total: 3800 so far.\n",
      "Fetched 100 stories. Total: 3900 so far.\n",
      "Fetched 100 stories. Total: 4000 so far.\n",
      "Fetched 100 stories. Total: 4100 so far.\n",
      "Fetched 100 stories. Total: 4200 so far.\n",
      "Fetched 100 stories. Total: 4300 so far.\n",
      "Fetched 100 stories. Total: 4400 so far.\n",
      "Fetched 100 stories. Total: 4500 so far.\n",
      "Fetched 100 stories. Total: 4600 so far.\n",
      "Fetched 100 stories. Total: 4700 so far.\n",
      "Fetched 100 stories. Total: 4800 so far.\n",
      "Fetched 100 stories. Total: 4900 so far.\n",
      "Fetched 100 stories. Total: 5000 so far.\n",
      "Fetched 100 stories. Total: 5100 so far.\n",
      "Fetched 100 stories. Total: 5200 so far.\n",
      "Fetched 100 stories. Total: 5300 so far.\n",
      "Fetched 100 stories. Total: 5400 so far.\n",
      "Fetched 100 stories. Total: 5500 so far.\n",
      "Fetched 100 stories. Total: 5600 so far.\n",
      "Fetched 100 stories. Total: 5700 so far.\n",
      "Fetched 100 stories. Total: 5800 so far.\n",
      "Fetched 100 stories. Total: 5900 so far.\n",
      "Fetched 100 stories. Total: 6000 so far.\n",
      "Fetched 100 stories. Total: 6100 so far.\n",
      "Fetched 100 stories. Total: 6200 so far.\n",
      "Fetched 100 stories. Total: 6300 so far.\n",
      "Fetched 100 stories. Total: 6400 so far.\n",
      "Fetched 100 stories. Total: 6500 so far.\n",
      "Fetched 100 stories. Total: 6600 so far.\n",
      "Fetched 100 stories. Total: 6700 so far.\n",
      "Fetched 100 stories. Total: 6800 so far.\n",
      "Fetched 100 stories. Total: 6900 so far.\n",
      "Fetched 100 stories. Total: 7000 so far.\n",
      "Fetched 100 stories. Total: 7100 so far.\n",
      "Fetched 100 stories. Total: 7200 so far.\n",
      "Fetched 100 stories. Total: 7300 so far.\n",
      "Fetched 100 stories. Total: 7400 so far.\n",
      "Fetched 100 stories. Total: 7500 so far.\n",
      "Fetched 100 stories. Total: 7600 so far.\n",
      "Fetched 100 stories. Total: 7700 so far.\n",
      "Fetched 100 stories. Total: 7800 so far.\n",
      "Fetched 100 stories. Total: 7900 so far.\n",
      "Fetched 100 stories. Total: 8000 so far.\n",
      "Fetched 100 stories. Total: 8100 so far.\n",
      "Fetched 100 stories. Total: 8200 so far.\n",
      "Fetched 100 stories. Total: 8300 so far.\n",
      "Fetched 100 stories. Total: 8400 so far.\n",
      "Fetched 100 stories. Total: 8500 so far.\n",
      "Fetched 100 stories. Total: 8600 so far.\n",
      "Fetched 100 stories. Total: 8700 so far.\n",
      "Fetched 100 stories. Total: 8800 so far.\n",
      "Fetched 100 stories. Total: 8900 so far.\n",
      "Fetched 100 stories. Total: 9000 so far.\n",
      "Fetched 100 stories. Total: 9100 so far.\n",
      "Fetched 100 stories. Total: 9200 so far.\n",
      "Fetched 100 stories. Total: 9300 so far.\n",
      "Fetched 100 stories. Total: 9400 so far.\n",
      "Fetched 100 stories. Total: 9500 so far.\n",
      "Fetched 100 stories. Total: 9600 so far.\n",
      "Fetched 100 stories. Total: 9700 so far.\n",
      "Fetched 100 stories. Total: 9800 so far.\n",
      "Fetched 100 stories. Total: 9900 so far.\n",
      "Fetched 100 stories. Total: 10000 so far.\n",
      "Reached limit of 10000 stories\n",
      "Next page cursor: Ny4zOTQyMzQ3LDMxODIzMjM4Njc=\n",
      "Removing trailing comma.\n"
     ]
    }
   ],
   "source": [
    "append_to_file = '../data/documents_2.json'\n",
    "limit = 10_000\n",
    "opts = opts_2\n",
    "# Note: Set the following manually if restarting kernel\n",
    "# opts['cursor'] = '*' # (for opts_1)\n",
    "# opts['cursor'] = '*' # (for opts_2)\n",
    "print(f'Fetching {limit} stories to file {append_to_file}. Starting at page {opts.get(\"cursor\") or 0}...')\n",
    "fetched = fetch_stories_to_file(opts, append_to_file, limit)\n",
    "\n",
    "# Note: this may eventually result in a JSON file that is rather large;\n",
    "# in that case we'll need to use some iterative approach to process it (e.g. ijson)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "opts['aql']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(append_to_file, 'r') as f:\n",
    "    stories = json.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(stories)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "documents = {story['document_id']: story for story in stories}\n",
    "len(documents)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
